{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LinusBach/SentimentAnalysis/blob/main/sentiAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FL2Mbm_sxwb"
   },
   "source": [
    "# Simple sentiment analysis\n",
    "\n",
    "Sentiment analysis, using iMDB database\n",
    "\n",
    "First, implement and train a feedforward NN model with TF-IDF. And then train your\n",
    "model using word2vec embedding. Report both training and development accuracy on\n",
    "the dataset. Try to use stochastic gradient descent or (mini-batch) stochastic gradient\n",
    "descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires word2vec model https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (23.5.9)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.23.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.4.10)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.54.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk) (2023.5.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (6.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpITqxI8ugWq"
   },
   "source": [
    "### imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OiLsmKu-ujrK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "\n",
    "# minimum ocuurences for a word to be regarded\n",
    "VOCAB_SIZE = 5000\n",
    "# number of most frequent words to be disregarded\n",
    "HIGHER_CUTOFF = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YruF5M9t3-0"
   },
   "source": [
    "### load dataset into memory\n",
    "return a list of docs and a list of respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YOaqXlBmtssb"
   },
   "outputs": [],
   "source": [
    "def load_data (filename):\n",
    "  content = list()\n",
    "  labels = list()\n",
    "\n",
    "  has_header = True\n",
    "  # detect if file has a header\n",
    "  # with open(filename, 'r') as file:\n",
    "  #   sample = file.read(64)\n",
    "  #   has_header = csv.Sniffer().has_header(sample)\n",
    "\n",
    "  with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    # skip first line if file has a header \n",
    "    if has_header:\n",
    "      next(reader)\n",
    "    for c, l in reader:\n",
    "      content.append(c)\n",
    "      labels.append(l)\n",
    "  return content, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsvSHK0Bt99T"
   },
   "source": [
    "### turn a dataset into clean tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N0Ig0SQvuJvd"
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "  corpus = list()\n",
    "  corp_voc = dict()\n",
    "  stop = nltk.corpus.stopwords.words(\"english\")\n",
    "  # regex tokenizer, find words, numbers and words containing '\n",
    "  tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+(?:'\\w)?\")\n",
    "  for doc in data:\n",
    "    doc = tokenizer.tokenize(doc)\n",
    "    doc_cleaned = dict()\n",
    "    for tok in doc:\n",
    "      # make all words lower case\n",
    "      tok = tok.lower()\n",
    "      # filter out numbers \n",
    "      if not tok.isdigit() and tok not in stop:\n",
    "        # add clean token to document\n",
    "        if tok in doc_cleaned:\n",
    "          doc_cleaned[tok] += 1\n",
    "        else:\n",
    "          doc_cleaned[tok] = 1\n",
    "    for tok in doc_cleaned.keys():\n",
    "    # increase corpus vocabulary\n",
    "        if tok in corp_voc:\n",
    "          corp_voc[tok] += 1\n",
    "        else:\n",
    "          corp_voc[tok] = 1\n",
    "    corpus.append(doc_cleaned)\n",
    "  return corpus, corp_voc\n",
    "\n",
    "# filter all words out of a corpus that are not in a vocabulary\n",
    "def get_filtered_corpus(corpus, vocab):\n",
    "  clean_corpus = list()\n",
    "  for doc in corpus:\n",
    "    clean_doc = dict()\n",
    "    for tok in doc:\n",
    "      if tok in vocab:\n",
    "        clean_doc[tok] = doc[tok]\n",
    "    clean_corpus.append(clean_doc)\n",
    "  return clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZInuMNEMuPVI"
   },
   "source": [
    "### preprocess the dataset\n",
    "\n",
    "tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E-XqdPRJuSA_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# corpus must be a list of dicts of form (token: occurences)\n",
    "# vocab must be a dict of form (token: documents in corpus containing token)\n",
    "def preprocess_tf_idf(corpus, vocab):\n",
    "  processed = np.zeros((len(corpus), len(vocab)))\n",
    "  idf = get_idf(vocab)\n",
    "  token_order = {tok: i for i, tok, in enumerate(sorted(vocab.keys()))}\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    tf = get_tf(doc)\n",
    "    for tok in set(doc):\n",
    "      tok_pos = token_order[tok]\n",
    "      processed[n_doc][tok_pos] = tf[tok]*idf[tok]\n",
    "  return processed\n",
    "\n",
    "def get_tf(doc):\n",
    "  tf = dict()\n",
    "  for tok, occ in doc.items():\n",
    "    tf[tok] = occ / len(doc)\n",
    "  return tf\n",
    "\n",
    "def get_idf(corp_voc):\n",
    "  idf = dict()\n",
    "  for tok, docs_containing in corp_voc.items():\n",
    "    idf[tok] = np.log10(len(corp_voc) / docs_containing)\n",
    "  return idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semantic embeddings\n",
    "pooling word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sem_vec(corpus, vocab_vectors):\n",
    "  processed = list()\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    vecs = [occ*vocab_vectors[tok] for tok, occ in doc.items() if tok in vocab_vectors]\n",
    "    processed.append(np.mean(vecs, axis=0))\n",
    "  return np.array(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHdoEQ-YuSuL"
   },
   "source": [
    "### define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2XiiBtEfuUoP"
   },
   "outputs": [],
   "source": [
    "def define_model(input_dim):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(layers.Dense(511, activation='relu'))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(255, activation='relu'))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(127, activation='relu'))\n",
    "  model.add(layers.Dropout(0.3))\n",
    "  model.add(layers.Dense(63, activation='relu'))\n",
    "  model.add(layers.Dense(2, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjvO8JTquZM9"
   },
   "source": [
    "### classify a review as negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ZeFui2DpudfA"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, doc):\n",
    "  return model.predict(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPMxAk0vSAiB"
   },
   "source": [
    "## Running the model\n",
    "\n",
    "Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-XH0prjdjST",
    "outputId": "892db3e7-1b92-4733-ebea-a32072b2459e"
   },
   "outputs": [],
   "source": [
    "# read data from file\n",
    "raw_data, labels = load_data(\"Train.csv\")\n",
    "full_corpus, full_vocab = clean_data(raw_data)\n",
    "# select part of vocabulary\n",
    "frequencies = sorted(full_vocab.items(), key=lambda x : x[1], reverse=True)\n",
    "vocab = {x[0] : x[1] for x in frequencies[HIGHER_CUTOFF:HIGHER_CUTOFF+VOCAB_SIZE]}\n",
    "corpus = get_filtered_corpus(full_corpus, vocab.keys())\n",
    "# process labels\n",
    "one_hot_labels = keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKsX4Kkb44ya"
   },
   "source": [
    "load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "RsmwRmA5_UJv"
   },
   "outputs": [],
   "source": [
    "v_raw, v_labels = load_data(\"Valid.csv\")\n",
    "full_valid_corpus, _ = clean_data(v_raw)\n",
    "valid_corpus = get_filtered_corpus(full_valid_corpus, vocab)\n",
    "valid_labels = keras.utils.to_categorical(v_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0IuRI2fC6oTC"
   },
   "outputs": [],
   "source": [
    "t_raw, t_labels = load_data(\"Test.csv\")\n",
    "full_test_corpus, _ = clean_data(t_raw)\n",
    "test_corpus = get_filtered_corpus(full_test_corpus, vocab)\n",
    "test_labels = keras.utils.to_categorical(t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on tf-idf data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf-idf representation of data\n",
    "tfidf_data = preprocess_tf_idf(corpus, vocab)\n",
    "test_data = preprocess_tf_idf(test_corpus, vocab)\n",
    "valid_data = preprocess_tf_idf(valid_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF7wNTALwMDp",
    "outputId": "6f707032-f32d-4a80-d6a3-b5cf74b7bbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 6s 5ms/step - loss: 0.1124 - accuracy: 0.8376 - val_loss: 0.0940 - val_accuracy: 0.8716\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.0738 - accuracy: 0.9014 - val_loss: 0.0902 - val_accuracy: 0.8748\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0623 - accuracy: 0.9201 - val_loss: 0.0925 - val_accuracy: 0.8784\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0494 - accuracy: 0.9397 - val_loss: 0.0941 - val_accuracy: 0.8766\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0367 - accuracy: 0.9564 - val_loss: 0.0998 - val_accuracy: 0.8760\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0290 - accuracy: 0.9668 - val_loss: 0.1113 - val_accuracy: 0.8678\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0236 - accuracy: 0.9736 - val_loss: 0.1057 - val_accuracy: 0.8786\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0206 - accuracy: 0.9773 - val_loss: 0.1124 - val_accuracy: 0.8710\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0196 - accuracy: 0.9779 - val_loss: 0.1103 - val_accuracy: 0.8728\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0173 - accuracy: 0.9805 - val_loss: 0.1122 - val_accuracy: 0.8714\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0167 - accuracy: 0.9812 - val_loss: 0.1079 - val_accuracy: 0.8802\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 6s 4ms/step - loss: 0.0150 - accuracy: 0.9831 - val_loss: 0.1092 - val_accuracy: 0.8756\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0143 - accuracy: 0.9839 - val_loss: 0.1167 - val_accuracy: 0.8696\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0130 - accuracy: 0.9855 - val_loss: 0.1078 - val_accuracy: 0.8766\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0130 - accuracy: 0.9854 - val_loss: 0.1112 - val_accuracy: 0.8778\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0125 - accuracy: 0.9861 - val_loss: 0.1170 - val_accuracy: 0.8712\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0120 - accuracy: 0.9865 - val_loss: 0.1119 - val_accuracy: 0.8730\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0111 - accuracy: 0.9879 - val_loss: 0.1132 - val_accuracy: 0.8732\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0109 - accuracy: 0.9878 - val_loss: 0.1107 - val_accuracy: 0.8742\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 5s 4ms/step - loss: 0.0108 - accuracy: 0.9883 - val_loss: 0.1106 - val_accuracy: 0.8776\n",
      "begin evaluation\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.8720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11472126096487045, 0.871999979019165]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "tfidf_model = define_model(VOCAB_SIZE,)\n",
    "tfidf_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "tfidf_model.fit(\n",
    "    tfidf_data,\n",
    "    one_hot_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "tfidf_model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on word2vec embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get semantic embeddings of data\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "vocab_vecs = {tok : word2vec_model[tok] for tok in vocab.keys() if tok in word2vec_model.key_to_index}\n",
    "w2v_train_data = preprocess_sem_vec(corpus, vocab_vecs)\n",
    "w2v_valid_data = preprocess_sem_vec(valid_corpus, vocab_vecs)\n",
    "w2v_test_data = preprocess_sem_vec(test_corpus, vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.4072 - accuracy: 0.8143 - val_loss: 0.3458 - val_accuracy: 0.8548\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3618 - accuracy: 0.8428 - val_loss: 0.3386 - val_accuracy: 0.8592\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3517 - accuracy: 0.8472 - val_loss: 0.3332 - val_accuracy: 0.8598\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3482 - accuracy: 0.8489 - val_loss: 0.3295 - val_accuracy: 0.8600\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3436 - accuracy: 0.8513 - val_loss: 0.3329 - val_accuracy: 0.8554\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3392 - accuracy: 0.8527 - val_loss: 0.3329 - val_accuracy: 0.8648\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3360 - accuracy: 0.8540 - val_loss: 0.3287 - val_accuracy: 0.8584\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3342 - accuracy: 0.8555 - val_loss: 0.3287 - val_accuracy: 0.8576\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3323 - accuracy: 0.8551 - val_loss: 0.3253 - val_accuracy: 0.8614\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3271 - accuracy: 0.8593 - val_loss: 0.3282 - val_accuracy: 0.8572\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3265 - accuracy: 0.8605 - val_loss: 0.3225 - val_accuracy: 0.8638\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3235 - accuracy: 0.8607 - val_loss: 0.3197 - val_accuracy: 0.8616\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3211 - accuracy: 0.8627 - val_loss: 0.3279 - val_accuracy: 0.8588\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3192 - accuracy: 0.8623 - val_loss: 0.3290 - val_accuracy: 0.8596\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3159 - accuracy: 0.8651 - val_loss: 0.3327 - val_accuracy: 0.8492\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3146 - accuracy: 0.8634 - val_loss: 0.3227 - val_accuracy: 0.8624\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3117 - accuracy: 0.8660 - val_loss: 0.3342 - val_accuracy: 0.8570\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3089 - accuracy: 0.8684 - val_loss: 0.3176 - val_accuracy: 0.8662\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3093 - accuracy: 0.8662 - val_loss: 0.3215 - val_accuracy: 0.8634\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.3039 - accuracy: 0.8698 - val_loss: 0.3231 - val_accuracy: 0.8624\n",
      "begin evaluation\n",
      "157/157 [==============================] - 0s 948us/step - loss: 0.3334 - accuracy: 0.8612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3334038555622101, 0.8611999750137329]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "w2v_model = define_model(300,)\n",
    "w2v_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "w2v_model.fit(\n",
    "    w2v_train_data,\n",
    "    one_hot_labels,\n",
    "    validation_data=(w2v_valid_data, valid_labels),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "w2v_model.evaluate(w2v_test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPun9gmYIf9t/BZSermdCJL",
   "include_colab_link": true,
   "mount_file_id": "1DkZU-n7fDBOIj7zec3xzfRXH0jZgeOaV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
