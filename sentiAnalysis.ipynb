{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LinusBach/SentimentAnalysis/blob/main/sentiAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FL2Mbm_sxwb"
   },
   "source": [
    "# Simple sentiment analysis\n",
    "\n",
    "Sentiment analysis, using iMDB database\n",
    "\n",
    "First, implement and train a feedforward NN model with TF-IDF. And then train your\n",
    "model using word2vec embedding. Report both training and development accuracy on\n",
    "the dataset. Try to use stochastic gradient descent or (mini-batch) stochastic gradient\n",
    "descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpITqxI8ugWq"
   },
   "source": [
    "### imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiLsmKu-ujrK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "import csv\n",
    "import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YruF5M9t3-0"
   },
   "source": [
    "### load dataset into memory\n",
    "return a list of docs and a list of respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOaqXlBmtssb"
   },
   "outputs": [],
   "source": [
    "def load_data (filename):\n",
    "  content = list()\n",
    "  labels = list()\n",
    "\n",
    "  has_header = True\n",
    "  # detect if file has a header\n",
    "  # with open(filename, 'r') as file:\n",
    "  #   sample = file.read(64)\n",
    "  #   has_header = csv.Sniffer().has_header(sample)\n",
    "\n",
    "  with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    # skip first line if file has a header \n",
    "    if has_header:\n",
    "      next(reader)\n",
    "    for c, l in reader:\n",
    "      content.append(c)\n",
    "      labels.append(l)\n",
    "  return content, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsvSHK0Bt99T"
   },
   "source": [
    "### turn a dataset into clean tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0Ig0SQvuJvd"
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "  corpus = list()\n",
    "  corp_voc = dict()\n",
    "  stop = nltk.corpus.stopwords.words(\"english\")\n",
    "  # regex tokenizer, find words, numbers and words containing '\n",
    "  tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+(?:'\\w)?\")\n",
    "  for doc in data:\n",
    "    doc = tokenizer.tokenize(doc)\n",
    "    doc_cleaned = dict()\n",
    "    for tok in doc:\n",
    "      # make all words lower case\n",
    "      tok = tok.lower()\n",
    "      # filter out numbers \n",
    "      if not tok.isdigit() and tok not in stop:\n",
    "        # add clean token to document\n",
    "        if tok in doc_cleaned:\n",
    "          doc_cleaned[tok] += 1\n",
    "        else:\n",
    "          doc_cleaned[tok] = 1\n",
    "    for tok in doc_cleaned.keys():\n",
    "    # increase corpus vocabulary\n",
    "        if tok in corp_voc:\n",
    "          corp_voc[tok] += 1\n",
    "        else:\n",
    "          corp_voc[tok] = 1\n",
    "    corpus.append(doc_cleaned)\n",
    "  return corpus, corp_voc\n",
    "\n",
    "# filter all words out of a corpus that are not in a vocabulary\n",
    "def get_filtered_corpus(corpus, vocab):\n",
    "  clean_corpus = list()\n",
    "  for doc in corpus:\n",
    "    clean_doc = dict()\n",
    "    for tok in doc:\n",
    "      if tok in vocab:\n",
    "        clean_doc[tok] = doc[tok]\n",
    "    clean_corpus.append(clean_doc)\n",
    "  return clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZInuMNEMuPVI"
   },
   "source": [
    "### preprocess the dataset\n",
    "\n",
    "tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-XqdPRJuSA_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# corpus must be a list of dicts of form (token: occurences)\n",
    "# vocab must be a dict of form (token: documents in corpus containing token)\n",
    "def preprocess_tf_idf(corpus, vocab):\n",
    "  processed = np.zeros((len(corpus), len(vocab)))\n",
    "  idf = get_idf(vocab)\n",
    "  token_order = {tok: i for i, tok, in enumerate(sorted(vocab.keys()))}\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    tf = get_tf(doc)\n",
    "    for tok in set(doc):\n",
    "      tok_pos = token_order[tok]\n",
    "      processed[n_doc][tok_pos] = tf[tok]*idf[tok]\n",
    "  return processed\n",
    "\n",
    "def get_tf(doc):\n",
    "  tf = dict()\n",
    "  for tok, occ in doc.items():\n",
    "    tf[tok] = occ / len(doc)\n",
    "  return tf\n",
    "\n",
    "def get_idf(corp_voc):\n",
    "  idf = dict()\n",
    "  for tok, docs_containing in corp_voc.items():\n",
    "    idf[tok] = np.log10(len(corp_voc) / docs_containing)\n",
    "  return idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semantic embeddings\n",
    "pooling word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sem_vec(corpus):\n",
    "  processed = list()\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    vecs = [occ*model[tok] for tok, occ in doc.items() if tok in model.key_to_index]\n",
    "    processed.append(np.mean(vecs, axis=0))\n",
    "  return np.array(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHdoEQ-YuSuL"
   },
   "source": [
    "### define the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XiiBtEfuUoP"
   },
   "outputs": [],
   "source": [
    "def define_model(input_dim):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(layers.Dense(511, input_dim=input_dim, activation='relu'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(255, activation='relu'))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(127, activation='relu'))\n",
    "  model.add(layers.Dense(2, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjvO8JTquZM9"
   },
   "source": [
    "### classify a review as negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeFui2DpudfA"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, doc):\n",
    "  return model.predict(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPMxAk0vSAiB"
   },
   "source": [
    "## Running the model\n",
    "\n",
    "Setting constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "# minimum ocuurences for a word to be regarded\n",
    "VOCAB_SIZE = 20000\n",
    "# number of most frequent words to be disregarded\n",
    "HIGHER_CUTOFF = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-XH0prjdjST",
    "outputId": "892db3e7-1b92-4733-ebea-a32072b2459e"
   },
   "outputs": [],
   "source": [
    "# read data from file\n",
    "raw_data, labels = load_data(\"Train.csv\")\n",
    "full_corpus, full_vocab = clean_data(raw_data)\n",
    "# select part of vocabulary\n",
    "frequencies = sorted(full_vocab.items(), key=lambda x : x[1], reverse=True)\n",
    "vocab = {x[0] : x[1] for x in frequencies[HIGHER_CUTOFF:HIGHER_CUTOFF+VOCAB_SIZE]}\n",
    "corpus = get_filtered_corpus(full_corpus, vocab.keys())\n",
    "# process labels\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKsX4Kkb44ya"
   },
   "source": [
    "load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsmwRmA5_UJv"
   },
   "outputs": [],
   "source": [
    "v_raw, v_labels = load_data(\"Valid.csv\")\n",
    "full_valid_corpus, _ = clean_data(v_raw)\n",
    "valid_corpus = get_filtered_corpus(full_valid_corpus, vocab)\n",
    "valid_labels = keras.utils.to_categorical(v_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IuRI2fC6oTC"
   },
   "outputs": [],
   "source": [
    "t_raw, t_labels = load_data(\"Test.csv\")\n",
    "full_test_corpus, _ = clean_data(t_raw)\n",
    "test_corpus = get_filtered_corpus(full_test_corpus, vocab)\n",
    "test_labels = keras.utils.to_categorical(t_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on tf-idf data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tf-idf representation of data\n",
    "tfidf_data = preprocess_tf_idf(corpus, vocab)\n",
    "test_data = preprocess_tf_idf(test_corpus, vocab)\n",
    "valid_data = preprocess_tf_idf(valid_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uF7wNTALwMDp",
    "outputId": "6f707032-f32d-4a80-d6a3-b5cf74b7bbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 59s 47ms/step - loss: 0.3144 - accuracy: 0.8626 - val_loss: 0.2616 - val_accuracy: 0.8958\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 59s 48ms/step - loss: 0.1471 - accuracy: 0.9450 - val_loss: 0.3074 - val_accuracy: 0.8866\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 57s 46ms/step - loss: 0.0673 - accuracy: 0.9733 - val_loss: 0.4385 - val_accuracy: 0.8744\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0248 - accuracy: 0.9896 - val_loss: 0.7215 - val_accuracy: 0.8822\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.9748 - val_accuracy: 0.8822\n",
      "Epoch 6/20\n",
      " 565/1250 [============>.................] - ETA: 30s - loss: 0.0075 - accuracy: 0.9980"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "tfidf_model = define_model(VOCAB_SIZE,)\n",
    "tfidf_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "tfidf_model.fit(\n",
    "    tfidf_data,\n",
    "    one_hot_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "tfidf_model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train a model on word2vec embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get semantic embeddings of data\n",
    "w2v_train_data = preprocess_sem_vec(corpus)\n",
    "w2v_valid_data = preprocess_sem_vec(valid_corpus)\n",
    "w2v_test_data = preprocess_sem_vec(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 4s 3ms/step - loss: 0.3884 - accuracy: 0.8260 - val_loss: 0.3462 - val_accuracy: 0.8456\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3511 - accuracy: 0.8473 - val_loss: 0.3395 - val_accuracy: 0.8534\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3413 - accuracy: 0.8541 - val_loss: 0.3236 - val_accuracy: 0.8624\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3377 - accuracy: 0.8549 - val_loss: 0.3246 - val_accuracy: 0.8616\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3319 - accuracy: 0.8565 - val_loss: 0.3191 - val_accuracy: 0.8612\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3266 - accuracy: 0.8593 - val_loss: 0.3344 - val_accuracy: 0.8596\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3230 - accuracy: 0.8619 - val_loss: 0.3362 - val_accuracy: 0.8512\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3192 - accuracy: 0.8636 - val_loss: 0.3199 - val_accuracy: 0.8662\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.3162 - accuracy: 0.8655 - val_loss: 0.3328 - val_accuracy: 0.8588\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3119 - accuracy: 0.8667 - val_loss: 0.3226 - val_accuracy: 0.8628\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3072 - accuracy: 0.8694 - val_loss: 0.3120 - val_accuracy: 0.8666\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.3044 - accuracy: 0.8712 - val_loss: 0.3089 - val_accuracy: 0.8662\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.3016 - accuracy: 0.8716 - val_loss: 0.3156 - val_accuracy: 0.8686\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2969 - accuracy: 0.8733 - val_loss: 0.3209 - val_accuracy: 0.8606\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2952 - accuracy: 0.8745 - val_loss: 0.3092 - val_accuracy: 0.8686\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2902 - accuracy: 0.8763 - val_loss: 0.3248 - val_accuracy: 0.8628\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2870 - accuracy: 0.8780 - val_loss: 0.3105 - val_accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2834 - accuracy: 0.8805 - val_loss: 0.3139 - val_accuracy: 0.8716\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2795 - accuracy: 0.8801 - val_loss: 0.3175 - val_accuracy: 0.8690\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.2751 - accuracy: 0.8817 - val_loss: 0.3274 - val_accuracy: 0.8626\n",
      "begin evaluation\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34957972168922424, 0.8497999906539917]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "w2v_model = define_model(300,)\n",
    "w2v_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "w2v_model.fit(\n",
    "    w2v_train_data,\n",
    "    one_hot_labels,\n",
    "    validation_data=(w2v_valid_data, valid_labels),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "w2v_model.evaluate(w2v_test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPun9gmYIf9t/BZSermdCJL",
   "include_colab_link": true,
   "mount_file_id": "1DkZU-n7fDBOIj7zec3xzfRXH0jZgeOaV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
