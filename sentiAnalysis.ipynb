{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LinusBach/SentimentAnalysis/blob/main/sentiAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FL2Mbm_sxwb"
      },
      "source": [
        "# Simple sentiment analysis\n",
        "\n",
        "Sentiment analysis, using iMDB database\n",
        "\n",
        "First, implement and train a feedforward NN model with TF-IDF. And then train your\n",
        "model using word2vec embedding. Report both training and development accuracy on\n",
        "the dataset. Try to use stochastic gradient descent or (mini-batch) stochastic gradient\n",
        "descent!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG1cIJBbsowv",
        "outputId": "603ea10f-7d80-41f8-cb55-0ac3db2fbfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sample_data/IMDB.zip\n",
            "  inflating: Test.csv                \n",
            "  inflating: Train.csv               \n",
            "  inflating: Valid.csv               \n"
          ]
        }
      ],
      "source": [
        "!cp drive/MyDrive/IMDB.zip sample_data/\n",
        "!unzip sample_data/IMDB.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpITqxI8ugWq"
      },
      "source": [
        "### imports and constants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiLsmKu-ujrK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import nltk\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20\n",
        "# minimum ocuurences for a word to be regarded\n",
        "VOCAB_SIZE = 5000\n",
        "# number of most frequent words to be disregarded\n",
        "HIGHER_CUTOFF = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YruF5M9t3-0"
      },
      "source": [
        "### load dataset into memory\n",
        "return a list of docs and a list of respective labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOaqXlBmtssb"
      },
      "outputs": [],
      "source": [
        "def load_data (filename):\n",
        "  content = list()\n",
        "  labels = list()\n",
        "\n",
        "  has_header = True\n",
        "  # detect if file has a header\n",
        "  # with open(filename, 'r') as file:\n",
        "  #   sample = file.read(64)\n",
        "  #   has_header = csv.Sniffer().has_header(sample)\n",
        "\n",
        "  with open(filename, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    # skip first line if file has a header \n",
        "    if has_header:\n",
        "      next(reader)\n",
        "    for c, l in reader:\n",
        "      content.append(c)\n",
        "      labels.append(l)\n",
        "  return content, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsvSHK0Bt99T"
      },
      "source": [
        "### turn a dataset into clean tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Ig0SQvuJvd"
      },
      "outputs": [],
      "source": [
        "def clean_data(data):\n",
        "  corpus = list()\n",
        "  corp_voc = dict()\n",
        "  # regex tokenizer, find words, numbers and words containing '\n",
        "  tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+(?:'\\w)?\")\n",
        "  for doc in data:\n",
        "    doc = tokenizer.tokenize(doc)\n",
        "    doc_cleaned = dict()\n",
        "    for tok in doc:\n",
        "      # make all words lower case\n",
        "      tok = tok.lower()\n",
        "      # filter out numbers \n",
        "      if not tok.isdigit():\n",
        "        # add clean token to document\n",
        "        if tok in doc_cleaned:\n",
        "          doc_cleaned[tok] += 1\n",
        "        else:\n",
        "          doc_cleaned[tok] = 1\n",
        "        # add clean token to corpus vocabulary\n",
        "        if tok in corp_voc:\n",
        "          corp_voc[tok] += 1\n",
        "        else:\n",
        "          corp_voc[tok] = 1\n",
        "    corpus.append(doc_cleaned)\n",
        "  return corpus, corp_voc\n",
        "\n",
        "# filter all words out of a corpus that are not in a vocabulary\n",
        "def get_filtered_corpus(corpus, vocab):\n",
        "  vocab = set(vocab)\n",
        "  clean_corpus = list()\n",
        "  for doc in corpus:\n",
        "    clean_doc = dict()\n",
        "    for tok in doc:\n",
        "      if tok in vocab:\n",
        "        clean_doc[tok] = doc[tok]\n",
        "    clean_corpus.append(clean_doc)\n",
        "  return clean_corpus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZInuMNEMuPVI"
      },
      "source": [
        "### preprocess the dataset\n",
        "\n",
        "some naive implementations, way to slow tho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-XqdPRJuSA_"
      },
      "outputs": [],
      "source": [
        "def get_tf(doc):\n",
        "  return {term : occ / len(doc) for term, occ in doc.items()}\n",
        "\n",
        "def get_idf(document, corpus):\n",
        "  {term : np.log10(len(corpus) / (sum(term in doc for doc in corpus) + 1)) for term in document.keys()}\n",
        "  idf = dict()\n",
        "  for n, term in enumerate(document):\n",
        "    if n%100 == 0:\n",
        "      print(f\"term {n} out of {len(document)}\")\n",
        "    docs_containing = sum(term in doc for doc in corpus)\n",
        "    idf[term] = np.log10(len(corpus) / (docs_containing + 1))\n",
        "  return idf\n",
        "\n",
        "def preprocess_tf_idf(corpus, vocab):\n",
        "  processed = np.zeros((len(corpus), len(vocab)))\n",
        "  idf = get_idf(vocab, corpus)\n",
        "  for n_doc, doc in enumerate(corpus):\n",
        "    print(f\"document {n_doc} out of {len(corpus)}\")\n",
        "    tf = get_tf(doc)\n",
        "    for tok in set(doc):\n",
        "      tok_pos = vocab.index(tok)\n",
        "      processed[n_doc][tok_pos] = tf[tok]*idf[tok]\n",
        "  return processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHdoEQ-YuSuL"
      },
      "source": [
        "### define the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XiiBtEfuUoP"
      },
      "outputs": [],
      "source": [
        "def define_model(input_dim):\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(511, input_dim=input_dim, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(255, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(127, activation='relu'))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(2, activation='softmax'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjvO8JTquZM9"
      },
      "source": [
        "### classify a review as negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeFui2DpudfA"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, doc):\n",
        "  return model.predict(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPMxAk0vSAiB"
      },
      "source": [
        "### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bZDqpVtSCv8"
      },
      "outputs": [],
      "source": [
        "raw_data, labels = load_data(\"Train.csv\")\n",
        "full_corpus, full_vocab = clean_data(raw_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-XH0prjdjST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "892db3e7-1b92-4733-ebea-a32072b2459e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 3, 'grew': 1, 'up': 1, 'b': 1, 'watching': 1, 'and': 3, 'loving': 1, 'the': 7, 'thunderbirds': 3, 'all': 2, 'my': 2, 'mates': 1, 'at': 1, 'school': 3, 'watched': 1, 'we': 2, 'played': 1, 'before': 1, 'during': 1, 'lunch': 1, 'after': 1, 'wanted': 2, 'to': 3, 'be': 3, 'virgil': 1, 'or': 1, 'scott': 1, 'no': 1, 'one': 2, 'alan': 1, 'counting': 1, 'down': 1, 'from': 1, 'became': 1, 'an': 1, 'art': 1, 'form': 1, 'took': 1, 'children': 1, 'see': 1, 'movie': 1, 'hoping': 1, 'they': 1, 'would': 1, 'get': 1, 'a': 5, 'glimpse': 1, 'of': 5, 'what': 1, 'loved': 1, 'as': 1, 'child': 1, 'how': 1, 'bitterly': 1, 'disappointing': 1, 'only': 1, 'high': 1, 'point': 1, 'was': 3, 'snappy': 1, 'theme': 1, 'tune': 1, 'not': 1, 'that': 1, 'it': 1, 'could': 1, 'compare': 1, 'with': 2, 'original': 1, 'score': 1, 'thankfully': 1, 'early': 1, 'saturday': 1, 'mornings': 1, 'television': 1, 'channel': 1, 'still': 1, 'plays': 1, 'reruns': 1, 'series': 1, 'gerry': 1, 'anderson': 1, 'his': 3, 'wife': 1, 'created': 1, 'jonatha': 1, 'frakes': 1, 'should': 1, 'hand': 1, 'in': 1, 'directors': 1, 'chair': 1, 'version': 1, 'completely': 1, 'hopeless': 1, 'waste': 1, 'film': 1, 'utter': 1, 'rubbish': 1, 'cgi': 1, 'remake': 1, 'may': 1, 'acceptable': 1, 'but': 1, 'replacing': 1, 'marionettes': 1, 'homo': 1, 'sapiens': 2, 'subsp': 1, 'huge': 1, 'error': 1, 'judgment': 1}\n",
            "{'grew': 1, 'up': 1, 'b': 1, 'watching': 1, 'loving': 1, 'all': 2, 'my': 2, 'at': 1, 'school': 3, 'watched': 1, 'we': 2, 'played': 1, 'before': 1, 'during': 1, 'after': 1, 'wanted': 2, 'be': 3, 'or': 1, 'scott': 1, 'no': 1, 'one': 2, 'alan': 1, 'down': 1, 'from': 1, 'became': 1, 'an': 1, 'art': 1, 'form': 1, 'took': 1, 'children': 1, 'see': 1, 'hoping': 1, 'they': 1, 'would': 1, 'get': 1, 'glimpse': 1, 'what': 1, 'loved': 1, 'child': 1, 'how': 1, 'disappointing': 1, 'only': 1, 'high': 1, 'point': 1, 'theme': 1, 'tune': 1, 'not': 1, 'could': 1, 'compare': 1, 'original': 1, 'score': 1, 'thankfully': 1, 'early': 1, 'saturday': 1, 'television': 1, 'channel': 1, 'still': 1, 'plays': 1, 'series': 1, 'anderson': 1, 'his': 3, 'wife': 1, 'created': 1, 'should': 1, 'hand': 1, 'directors': 1, 'chair': 1, 'version': 1, 'completely': 1, 'hopeless': 1, 'waste': 1, 'utter': 1, 'rubbish': 1, 'cgi': 1, 'remake': 1, 'may': 1, 'acceptable': 1, 'huge': 1}\n"
          ]
        }
      ],
      "source": [
        "frequencies = sorted(full_vocab.items(), key=lambda x : x[1], reverse=True)\n",
        "vocab = {x[0] : x[1] for x in frequencies[HIGHER_CUTOFF:HIGHER_CUTOFF+VOCAB_SIZE]}\n",
        "\n",
        "corpus = get_filtered_corpus(full_corpus, vocab.keys())\n",
        "print(corpus[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKsX4Kkb44ya"
      },
      "outputs": [],
      "source": [
        "data = preprocess_tf_idf(corpus, sorted(vocab.keys()))\n",
        "one_hot_labels = keras.utils.to_categorical(labels, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v_raw, v_labels = load_data(\"Valid.csv\")\n",
        "full_valid_corpus, _ = clean_data(v_raw)\n",
        "valid_corpus = get_filtered_corpus(full_valid_corpus, vocab.keys())\n",
        "valid_data = preprocess_tf_idf(valid_corpus, sorted(vocab.keys()))\n",
        "valid_labels = keras.utils.to_categorical(v_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "RsmwRmA5_UJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t_raw, t_labels = load_data(\"Test.csv\")\n",
        "full_test_corpus, _ = clean_data(t_raw)\n",
        "test_corpus = get_filtered_corpus(full_test_corpus, vocab.keys())\n",
        "test_data = preprocess_tf_idf(test_corpus, sorted(vocab.keys()))\n",
        "test_labels = keras.utils.to_categorical(t_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "0IuRI2fC6oTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(VOCAB_SIZE,)\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    data,\n",
        "    one_hot_labels,\n",
        "    validation_data=(valid_data, valid_labels),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF7wNTALwMDp",
        "outputId": "6f707032-f32d-4a80-d6a3-b5cf74b7bbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1250/1250 [==============================] - 62s 48ms/step - loss: 0.3154 - accuracy: 0.8645 - val_loss: 0.2754 - val_accuracy: 0.8860\n",
            "Epoch 2/20\n",
            "1250/1250 [==============================] - 64s 51ms/step - loss: 0.2266 - accuracy: 0.9090 - val_loss: 0.2935 - val_accuracy: 0.8812\n",
            "Epoch 3/20\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.1677 - accuracy: 0.9318 - val_loss: 0.3076 - val_accuracy: 0.8784\n",
            "Epoch 4/20\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.0800 - accuracy: 0.9704 - val_loss: 0.4842 - val_accuracy: 0.8784\n",
            "Epoch 5/20\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0219 - accuracy: 0.9929 - val_loss: 0.6662 - val_accuracy: 0.8744\n",
            "Epoch 6/20\n",
            "1250/1250 [==============================] - 59s 48ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.8521 - val_accuracy: 0.8808\n",
            "Epoch 7/20\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.9555 - val_accuracy: 0.8768\n",
            "Epoch 8/20\n",
            "1250/1250 [==============================] - 62s 49ms/step - loss: 0.0100 - accuracy: 0.9965 - val_loss: 0.7983 - val_accuracy: 0.8784\n",
            "Epoch 9/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.0068 - accuracy: 0.9977 - val_loss: 0.9815 - val_accuracy: 0.8740\n",
            "Epoch 10/20\n",
            "1250/1250 [==============================] - 58s 47ms/step - loss: 0.0066 - accuracy: 0.9977 - val_loss: 0.8401 - val_accuracy: 0.8756\n",
            "Epoch 11/20\n",
            "1250/1250 [==============================] - 62s 49ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.8167 - val_accuracy: 0.8784\n",
            "Epoch 12/20\n",
            "1250/1250 [==============================] - 61s 48ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.9186 - val_accuracy: 0.8784\n",
            "Epoch 13/20\n",
            "1250/1250 [==============================] - 59s 47ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.9814 - val_accuracy: 0.8814\n",
            "Epoch 14/20\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 1.1174 - val_accuracy: 0.8828\n",
            "Epoch 15/20\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.9934 - val_accuracy: 0.8762\n",
            "Epoch 16/20\n",
            "1250/1250 [==============================] - 56s 45ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 1.0789 - val_accuracy: 0.8712\n",
            "Epoch 17/20\n",
            "1250/1250 [==============================] - 58s 46ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 1.0809 - val_accuracy: 0.8732\n",
            "Epoch 18/20\n",
            "1250/1250 [==============================] - 61s 48ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.1307 - val_accuracy: 0.8764\n",
            "Epoch 19/20\n",
            "1250/1250 [==============================] - 61s 49ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.9858 - val_accuracy: 0.8764\n",
            "Epoch 20/20\n",
            "1250/1250 [==============================] - 60s 48ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 1.0826 - val_accuracy: 0.8806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.evaluate(test_data, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R9P4-swwuaN",
        "outputId": "f1a5eae3-c557-4c8a-a547-ef40ff831c13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 2s 12ms/step - loss: 0.9522 - accuracy: 0.8812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compute_loss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJTcAHt58nbV",
        "outputId": "a40ebad3-b84c-4326-9913-dda1c9c845fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1920928955078126e-11\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1DkZU-n7fDBOIj7zec3xzfRXH0jZgeOaV",
      "authorship_tag": "ABX9TyPun9gmYIf9t/BZSermdCJL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}