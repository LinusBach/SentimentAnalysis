{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LinusBach/SentimentAnalysis/blob/main/sentiAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FL2Mbm_sxwb"
   },
   "source": [
    "# Simple sentiment analysis\n",
    "\n",
    "Sentiment analysis, using iMDB database\n",
    "\n",
    "First, implement and train a feedforward NN model with TF-IDF. And then train your\n",
    "model using word2vec embedding. Report both training and development accuracy on\n",
    "the dataset. Try to use stochastic gradient descent or (mini-batch) stochastic gradient\n",
    "descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-12T09:13:20.368413Z",
     "start_time": "2023-05-12T09:13:15.922079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (2.11.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.42.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (4.5.0)\r\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (16.0.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (2.11.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (2.0)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.24.3)\r\n",
      "Requirement already satisfied: setuptools in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (66.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (3.7.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: packaging in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (23.0)\r\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (3.19.6)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.35.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.6.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.4)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.6.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.29.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.7.2)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.2)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\r\n",
      "Requirement already satisfied: nltk in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: tqdm in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from nltk) (4.65.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from nltk) (2023.5.5)\r\n",
      "Requirement already satisfied: joblib in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from nltk) (1.2.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/uni/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (4.3.1)\r\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from gensim) (1.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from gensim) (1.24.3)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/uni/anaconda3/envs/tf/lib/python3.10/site-packages (from gensim) (6.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpITqxI8ugWq"
   },
   "source": [
    "### imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OiLsmKu-ujrK",
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:33.761614Z",
     "start_time": "2023-05-15T13:33:24.816384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/uni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import csv\n",
    "import numpy as np\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YruF5M9t3-0"
   },
   "source": [
    "### load dataset into memory\n",
    "return a list of docs and a list of respective labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YOaqXlBmtssb",
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:33.776992Z",
     "start_time": "2023-05-15T13:33:33.762118Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data (filename):\n",
    "  content = list()\n",
    "  labels = list()\n",
    "\n",
    "  has_header = True\n",
    "  # detect if file has a header\n",
    "  # with open(filename, 'r') as file:\n",
    "  #   sample = file.read(64)\n",
    "  #   has_header = csv.Sniffer().has_header(sample)\n",
    "\n",
    "  with open(filename, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    # skip first line if file has a header \n",
    "    if has_header:\n",
    "      next(reader)\n",
    "    for c, l in reader:\n",
    "      content.append(c)\n",
    "      labels.append(l)\n",
    "  return content, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsvSHK0Bt99T"
   },
   "source": [
    "### turn a dataset into clean tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N0Ig0SQvuJvd",
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:33.795940Z",
     "start_time": "2023-05-15T13:33:33.765736Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "  corpus = list()\n",
    "  corp_voc = dict()\n",
    "  stop = nltk.corpus.stopwords.words(\"english\")\n",
    "  # regex tokenizer, find words, numbers and words containing '\n",
    "  tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+(?:'\\w)?\")\n",
    "  for doc in data:\n",
    "    doc = tokenizer.tokenize(doc)\n",
    "    doc_cleaned = dict()\n",
    "    for tok in doc:\n",
    "      # make all words lower case\n",
    "      tok = tok.lower()\n",
    "      # filter out numbers \n",
    "      if not tok.isdigit() and tok not in stop:\n",
    "        # add clean token to document\n",
    "        if tok in doc_cleaned:\n",
    "          doc_cleaned[tok] += 1\n",
    "        else:\n",
    "          doc_cleaned[tok] = 1\n",
    "    for tok in doc_cleaned.keys():\n",
    "    # increase corpus vocabulary\n",
    "        if tok in corp_voc:\n",
    "          corp_voc[tok] += 1\n",
    "        else:\n",
    "          corp_voc[tok] = 1\n",
    "    corpus.append(doc_cleaned)\n",
    "  return corpus, corp_voc\n",
    "\n",
    "# filter all words out of a corpus that are not in a vocabulary\n",
    "def get_filtered_corpus(corpus, vocab):\n",
    "  clean_corpus = list()\n",
    "  for doc in corpus:\n",
    "    clean_doc = dict()\n",
    "    for tok in doc:\n",
    "      if tok in vocab:\n",
    "        clean_doc[tok] = doc[tok]\n",
    "    clean_corpus.append(clean_doc)\n",
    "  return clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZInuMNEMuPVI"
   },
   "source": [
    "### preprocess the dataset\n",
    "\n",
    "tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E-XqdPRJuSA_",
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:33.796231Z",
     "start_time": "2023-05-15T13:33:33.770464Z"
    }
   },
   "outputs": [],
   "source": [
    "# corpus must be a list of dicts of form (token: occurences)\n",
    "# vocab must be a dict of form (token: documents in corpus containing token)\n",
    "def preprocess_tf_idf(corpus, vocab):\n",
    "  processed = np.zeros((len(corpus), len(vocab)))\n",
    "  idf = get_idf(vocab)\n",
    "  token_order = {tok: i for i, tok, in enumerate(sorted(vocab.keys()))}\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    tf = get_tf(doc)\n",
    "    for tok in set(doc):\n",
    "      tok_pos = token_order[tok]\n",
    "      processed[n_doc][tok_pos] = tf[tok]*idf[tok]\n",
    "  return processed\n",
    "\n",
    "def get_tf(doc):\n",
    "  tf = dict()\n",
    "  for tok, occ in doc.items():\n",
    "    tf[tok] = occ / len(doc)\n",
    "  return tf\n",
    "\n",
    "def get_idf(corp_voc):\n",
    "  idf = dict()\n",
    "  for tok, docs_containing in corp_voc.items():\n",
    "    idf[tok] = np.log10(len(corp_voc) / docs_containing)\n",
    "  return idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "semantic embeddings\n",
    "pooling word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:33.796348Z",
     "start_time": "2023-05-15T13:33:33.774757Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_sem_vec(corpus):\n",
    "  processed = list()\n",
    "  model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "  total_used = 0\n",
    "  total_available = 0\n",
    "  for n_doc, doc in enumerate(corpus):\n",
    "    vecs = [occ*model[tok] for tok, occ in doc.items() if tok in model.key_to_index]\n",
    "    total_used += len(vecs)\n",
    "    total_available += len(doc.items())\n",
    "    processed.append(np.mean(vecs, axis=0))\n",
    "\n",
    "  return np.array(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "load training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# read data from file\n",
    "raw_data, labels = load_data(\"Train.csv\")\n",
    "full_corpus, full_vocab = clean_data(raw_data)\n",
    "\n",
    "# select part of vocabulary\n",
    "frequencies = sorted(full_vocab.items(), key=lambda x : x[1], reverse=True)\n",
    "# number of most common non-stop words that we consider\n",
    "VOCAB_SIZE = 20000\n",
    "# number of most frequent words to be disregarded, but this is now irrelevant due to the removal of stop words\n",
    "HIGHER_CUTOFF = 0\n",
    "vocab = {x[0] : x[1] for x in frequencies[HIGHER_CUTOFF:HIGHER_CUTOFF+VOCAB_SIZE]}\n",
    "\n",
    "corpus = get_filtered_corpus(full_corpus, vocab.keys())\n",
    "# process labels\n",
    "taining_labels = np.array([0 if int(label) == 0 else 1 for label in labels]) # keras.utils.to_categorical(labels, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:40:56.987042Z",
     "start_time": "2023-05-15T13:40:44.157813Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load validation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "v_raw, v_labels = load_data(\"Valid.csv\")\n",
    "full_valid_corpus, _ = clean_data(v_raw)\n",
    "valid_corpus = get_filtered_corpus(full_valid_corpus, vocab)\n",
    "valid_labels = np.array([0 if int(label) == 0 else 1 for label in v_labels]) # keras.utils.to_categorical(v_labels, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:47.727947Z",
     "start_time": "2023-05-15T13:33:46.064749Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load testing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "t_raw, t_labels = load_data(\"Test.csv\")\n",
    "full_test_corpus, _ = clean_data(t_raw)\n",
    "test_corpus = get_filtered_corpus(full_test_corpus, vocab)\n",
    "test_labels = np.array([0 if int(label) == 0 else 1 for label in t_labels]) # keras.utils.to_categorical(t_labels, num_classes=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:49.392534Z",
     "start_time": "2023-05-15T13:33:47.728446Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "define and preprocess training, testing and validation data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# get tf-idf representation of data\n",
    "tfidf_data = preprocess_tf_idf(corpus, vocab)\n",
    "test_data = preprocess_tf_idf(test_corpus, vocab)\n",
    "valid_data = preprocess_tf_idf(valid_corpus, vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:33:51.659783Z",
     "start_time": "2023-05-15T13:33:49.467577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T13:41:48.395804Z",
     "start_time": "2023-05-15T13:40:56.988499Z"
    }
   },
   "outputs": [],
   "source": [
    "# get semantic embeddings of data\n",
    "w2v_train_data = preprocess_sem_vec(corpus)\n",
    "w2v_valid_data = preprocess_sem_vec(valid_corpus)\n",
    "w2v_test_data = preprocess_sem_vec(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### classify a review as negative or positive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def predict_sentiment(model, doc):\n",
    "  return model.predict(doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:34:39.857809Z",
     "start_time": "2023-05-15T13:34:39.854531Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### define the model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def define_tf_idf_model(dropout=0.2, l1=1e-5, l2=1e-4):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:34:39.859321Z",
     "start_time": "2023-05-15T13:34:39.855580Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the model\n",
    "\n",
    "Setting constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "TF_LEARNING_RATE = 3e-5\n",
    "TF_BATCH_SIZE = 512\n",
    "TF_EPOCHS = 100\n",
    "\n",
    "TF_DROPOUT = 0.25\n",
    "TF_L1 = 5e-6\n",
    "TF_L2 = 5e-5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:37:28.892093Z",
     "start_time": "2023-05-15T13:37:28.886165Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/100\n",
      "13586/40000 [=========>....................] - ETA: 35s - loss: 0.7272 - accuracy: 0.5144"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# train model\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin training\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m \u001B[43mtfidf_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtfidf_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtaining_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvalid_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTF_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTF_BATCH_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbegin evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     16\u001B[0m tfidf_model\u001B[38;5;241m.\u001B[39mevaluate(test_data, test_labels)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1650\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1642\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1643\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1644\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1648\u001B[0m ):\n\u001B[1;32m   1649\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1650\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1651\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1652\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[1;32m    133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1741\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1744\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1745\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1747\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m     args,\n\u001B[1;32m   1749\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1750\u001B[0m     executing_eagerly)\n\u001B[1;32m   1751\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    377\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 378\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    387\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    391\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# build model\n",
    "tfidf_model = define_tf_idf_model(dropout=TF_DROPOUT, l1=TF_L1, l2=TF_L2)\n",
    "tfidf_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=TF_LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "tfidf_model.fit(\n",
    "    tfidf_data,\n",
    "    taining_labels,\n",
    "    validation_data=(valid_data, valid_labels),\n",
    "    epochs=TF_EPOCHS,\n",
    "    batch_size=TF_BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "tfidf_model.evaluate(test_data, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:37:47.989185Z",
     "start_time": "2023-05-15T13:37:29.035846Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "train a model on word2vec embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def define_word2vec_model(dropout=0.4, l1=1e-5, l2=1e-4):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(128, activation='relu', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:42:41.453336Z",
     "start_time": "2023-05-15T13:42:41.447139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "W2V_LEARNING_RATE = 2e-4\n",
    "W2V_BATCH_SIZE = 512\n",
    "W2V_EPOCHS = 100\n",
    "\n",
    "W2V_DROPOUT = 0.1\n",
    "W2V_L1 = 2e-5\n",
    "W2V_L2 = 1e-4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:42:41.804635Z",
     "start_time": "2023-05-15T13:42:41.783359Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T13:43:17.537396Z",
     "start_time": "2023-05-15T13:42:41.948221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training\n",
      "Epoch 1/100\n",
      "79/79 [==============================] - 1s 5ms/step - loss: 0.8059 - accuracy: 0.7070 - val_loss: 0.7145 - val_accuracy: 0.7990\n",
      "Epoch 2/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.6342 - accuracy: 0.7951 - val_loss: 0.5574 - val_accuracy: 0.8148\n",
      "Epoch 3/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5480 - accuracy: 0.8124 - val_loss: 0.5084 - val_accuracy: 0.8326\n",
      "Epoch 4/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5172 - accuracy: 0.8229 - val_loss: 0.4844 - val_accuracy: 0.8396\n",
      "Epoch 5/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5017 - accuracy: 0.8276 - val_loss: 0.4733 - val_accuracy: 0.8440\n",
      "Epoch 6/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4903 - accuracy: 0.8300 - val_loss: 0.4593 - val_accuracy: 0.8500\n",
      "Epoch 7/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4823 - accuracy: 0.8306 - val_loss: 0.4543 - val_accuracy: 0.8476\n",
      "Epoch 8/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.8338 - val_loss: 0.4470 - val_accuracy: 0.8500\n",
      "Epoch 9/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4719 - accuracy: 0.8328 - val_loss: 0.4428 - val_accuracy: 0.8520\n",
      "Epoch 10/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4666 - accuracy: 0.8336 - val_loss: 0.4399 - val_accuracy: 0.8508\n",
      "Epoch 11/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4627 - accuracy: 0.8332 - val_loss: 0.4351 - val_accuracy: 0.8528\n",
      "Epoch 12/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4623 - accuracy: 0.8339 - val_loss: 0.4409 - val_accuracy: 0.8460\n",
      "Epoch 13/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4596 - accuracy: 0.8350 - val_loss: 0.4350 - val_accuracy: 0.8484\n",
      "Epoch 14/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8351 - val_loss: 0.4342 - val_accuracy: 0.8468\n",
      "Epoch 15/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8355 - val_loss: 0.4248 - val_accuracy: 0.8544\n",
      "Epoch 16/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4520 - accuracy: 0.8353 - val_loss: 0.4375 - val_accuracy: 0.8414\n",
      "Epoch 17/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4507 - accuracy: 0.8356 - val_loss: 0.4207 - val_accuracy: 0.8548\n",
      "Epoch 18/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4471 - accuracy: 0.8352 - val_loss: 0.4203 - val_accuracy: 0.8554\n",
      "Epoch 19/100\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.4466 - accuracy: 0.8350 - val_loss: 0.4179 - val_accuracy: 0.8550\n",
      "Epoch 20/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4445 - accuracy: 0.8338 - val_loss: 0.4169 - val_accuracy: 0.8562\n",
      "Epoch 21/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.8367 - val_loss: 0.4226 - val_accuracy: 0.8462\n",
      "Epoch 22/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.8364 - val_loss: 0.4132 - val_accuracy: 0.8572\n",
      "Epoch 23/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4395 - accuracy: 0.8353 - val_loss: 0.4115 - val_accuracy: 0.8582\n",
      "Epoch 24/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4395 - accuracy: 0.8360 - val_loss: 0.4102 - val_accuracy: 0.8574\n",
      "Epoch 25/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4362 - accuracy: 0.8378 - val_loss: 0.4172 - val_accuracy: 0.8450\n",
      "Epoch 26/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8375 - val_loss: 0.4070 - val_accuracy: 0.8572\n",
      "Epoch 27/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.8367 - val_loss: 0.4135 - val_accuracy: 0.8472\n",
      "Epoch 28/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4339 - accuracy: 0.8359 - val_loss: 0.4146 - val_accuracy: 0.8472\n",
      "Epoch 29/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8382 - val_loss: 0.4051 - val_accuracy: 0.8570\n",
      "Epoch 30/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4301 - accuracy: 0.8371 - val_loss: 0.4067 - val_accuracy: 0.8526\n",
      "Epoch 31/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8365 - val_loss: 0.4067 - val_accuracy: 0.8512\n",
      "Epoch 32/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8373 - val_loss: 0.4058 - val_accuracy: 0.8514\n",
      "Epoch 33/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.8388 - val_loss: 0.4056 - val_accuracy: 0.8496\n",
      "Epoch 34/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4258 - accuracy: 0.8381 - val_loss: 0.4049 - val_accuracy: 0.8510\n",
      "Epoch 35/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4251 - accuracy: 0.8382 - val_loss: 0.4005 - val_accuracy: 0.8550\n",
      "Epoch 36/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4233 - accuracy: 0.8379 - val_loss: 0.3975 - val_accuracy: 0.8590\n",
      "Epoch 37/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4208 - accuracy: 0.8397 - val_loss: 0.4014 - val_accuracy: 0.8512\n",
      "Epoch 38/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.8394 - val_loss: 0.3996 - val_accuracy: 0.8528\n",
      "Epoch 39/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4223 - accuracy: 0.8389 - val_loss: 0.3951 - val_accuracy: 0.8602\n",
      "Epoch 40/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4205 - accuracy: 0.8392 - val_loss: 0.3983 - val_accuracy: 0.8522\n",
      "Epoch 41/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8392 - val_loss: 0.3930 - val_accuracy: 0.8598\n",
      "Epoch 42/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4195 - accuracy: 0.8406 - val_loss: 0.3924 - val_accuracy: 0.8600\n",
      "Epoch 43/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4174 - accuracy: 0.8372 - val_loss: 0.3928 - val_accuracy: 0.8572\n",
      "Epoch 44/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4183 - accuracy: 0.8365 - val_loss: 0.3965 - val_accuracy: 0.8504\n",
      "Epoch 45/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4168 - accuracy: 0.8406 - val_loss: 0.3906 - val_accuracy: 0.8602\n",
      "Epoch 46/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4164 - accuracy: 0.8410 - val_loss: 0.3924 - val_accuracy: 0.8542\n",
      "Epoch 47/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4152 - accuracy: 0.8400 - val_loss: 0.3890 - val_accuracy: 0.8594\n",
      "Epoch 48/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4146 - accuracy: 0.8391 - val_loss: 0.3948 - val_accuracy: 0.8510\n",
      "Epoch 49/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4162 - accuracy: 0.8387 - val_loss: 0.3903 - val_accuracy: 0.8566\n",
      "Epoch 50/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.8420 - val_loss: 0.3871 - val_accuracy: 0.8606\n",
      "Epoch 51/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4123 - accuracy: 0.8398 - val_loss: 0.3931 - val_accuracy: 0.8528\n",
      "Epoch 52/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8408 - val_loss: 0.3870 - val_accuracy: 0.8584\n",
      "Epoch 53/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4112 - accuracy: 0.8407 - val_loss: 0.3859 - val_accuracy: 0.8590\n",
      "Epoch 54/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4091 - accuracy: 0.8419 - val_loss: 0.3853 - val_accuracy: 0.8596\n",
      "Epoch 55/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4081 - accuracy: 0.8437 - val_loss: 0.3843 - val_accuracy: 0.8598\n",
      "Epoch 56/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4086 - accuracy: 0.8416 - val_loss: 0.3853 - val_accuracy: 0.8588\n",
      "Epoch 57/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4079 - accuracy: 0.8412 - val_loss: 0.3839 - val_accuracy: 0.8592\n",
      "Epoch 58/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8428 - val_loss: 0.3851 - val_accuracy: 0.8566\n",
      "Epoch 59/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4053 - accuracy: 0.8423 - val_loss: 0.3833 - val_accuracy: 0.8588\n",
      "Epoch 60/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4044 - accuracy: 0.8406 - val_loss: 0.3812 - val_accuracy: 0.8604\n",
      "Epoch 61/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4044 - accuracy: 0.8432 - val_loss: 0.3810 - val_accuracy: 0.8614\n",
      "Epoch 62/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4059 - accuracy: 0.8397 - val_loss: 0.3816 - val_accuracy: 0.8602\n",
      "Epoch 63/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4026 - accuracy: 0.8425 - val_loss: 0.3818 - val_accuracy: 0.8592\n",
      "Epoch 64/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4056 - accuracy: 0.8414 - val_loss: 0.3842 - val_accuracy: 0.8566\n",
      "Epoch 65/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4031 - accuracy: 0.8429 - val_loss: 0.3807 - val_accuracy: 0.8602\n",
      "Epoch 66/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4040 - accuracy: 0.8414 - val_loss: 0.3832 - val_accuracy: 0.8562\n",
      "Epoch 67/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4035 - accuracy: 0.8416 - val_loss: 0.3799 - val_accuracy: 0.8610\n",
      "Epoch 68/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.8440 - val_loss: 0.3842 - val_accuracy: 0.8556\n",
      "Epoch 69/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4009 - accuracy: 0.8415 - val_loss: 0.3777 - val_accuracy: 0.8606\n",
      "Epoch 70/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4003 - accuracy: 0.8435 - val_loss: 0.3796 - val_accuracy: 0.8590\n",
      "Epoch 71/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.4007 - accuracy: 0.8421 - val_loss: 0.3813 - val_accuracy: 0.8578\n",
      "Epoch 72/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3993 - accuracy: 0.8444 - val_loss: 0.3767 - val_accuracy: 0.8612\n",
      "Epoch 73/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.8429 - val_loss: 0.3794 - val_accuracy: 0.8576\n",
      "Epoch 74/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3988 - accuracy: 0.8445 - val_loss: 0.3778 - val_accuracy: 0.8610\n",
      "Epoch 75/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3977 - accuracy: 0.8433 - val_loss: 0.3747 - val_accuracy: 0.8618\n",
      "Epoch 76/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3980 - accuracy: 0.8430 - val_loss: 0.3756 - val_accuracy: 0.8606\n",
      "Epoch 77/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3961 - accuracy: 0.8455 - val_loss: 0.3752 - val_accuracy: 0.8604\n",
      "Epoch 78/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8446 - val_loss: 0.3759 - val_accuracy: 0.8590\n",
      "Epoch 79/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8423 - val_loss: 0.3932 - val_accuracy: 0.8404\n",
      "Epoch 80/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3939 - accuracy: 0.8459 - val_loss: 0.3736 - val_accuracy: 0.8626\n",
      "Epoch 81/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3928 - accuracy: 0.8456 - val_loss: 0.3732 - val_accuracy: 0.8614\n",
      "Epoch 82/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8439 - val_loss: 0.3869 - val_accuracy: 0.8470\n",
      "Epoch 83/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3934 - accuracy: 0.8442 - val_loss: 0.3730 - val_accuracy: 0.8620\n",
      "Epoch 84/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3964 - accuracy: 0.8425 - val_loss: 0.3728 - val_accuracy: 0.8626\n",
      "Epoch 85/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8436 - val_loss: 0.3755 - val_accuracy: 0.8582\n",
      "Epoch 86/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3935 - accuracy: 0.8451 - val_loss: 0.3779 - val_accuracy: 0.8588\n",
      "Epoch 87/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8424 - val_loss: 0.3751 - val_accuracy: 0.8590\n",
      "Epoch 88/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8438 - val_loss: 0.3714 - val_accuracy: 0.8610\n",
      "Epoch 89/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3940 - accuracy: 0.8440 - val_loss: 0.3748 - val_accuracy: 0.8578\n",
      "Epoch 90/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3903 - accuracy: 0.8463 - val_loss: 0.3713 - val_accuracy: 0.8632\n",
      "Epoch 91/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3873 - accuracy: 0.8478 - val_loss: 0.3783 - val_accuracy: 0.8546\n",
      "Epoch 92/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3901 - accuracy: 0.8460 - val_loss: 0.3701 - val_accuracy: 0.8606\n",
      "Epoch 93/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3906 - accuracy: 0.8461 - val_loss: 0.3706 - val_accuracy: 0.8608\n",
      "Epoch 94/100\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3906 - accuracy: 0.8446 - val_loss: 0.3695 - val_accuracy: 0.8616\n",
      "Epoch 95/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3910 - accuracy: 0.8455 - val_loss: 0.3703 - val_accuracy: 0.8600\n",
      "Epoch 96/100\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8449 - val_loss: 0.3713 - val_accuracy: 0.8608\n",
      "Epoch 97/100\n",
      "79/79 [==============================] - 0s 5ms/step - loss: 0.3893 - accuracy: 0.8460 - val_loss: 0.3687 - val_accuracy: 0.8622\n",
      "Epoch 98/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3878 - accuracy: 0.8471 - val_loss: 0.3717 - val_accuracy: 0.8604\n",
      "Epoch 99/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3892 - accuracy: 0.8449 - val_loss: 0.3687 - val_accuracy: 0.8600\n",
      "Epoch 100/100\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.3848 - accuracy: 0.8475 - val_loss: 0.3670 - val_accuracy: 0.8622\n",
      "begin evaluation\n",
      "157/157 [==============================] - 0s 623us/step - loss: 0.3798 - accuracy: 0.8534\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.37983185052871704, 0.8533999919891357]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model\n",
    "w2v_model = define_word2vec_model(dropout=W2V_DROPOUT, l1=W2V_L1, l2=W2V_L2)\n",
    "w2v_model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=W2V_LEARNING_RATE),\n",
    "    metrics=['accuracy'])\n",
    "# train model\n",
    "print(\"begin training\")\n",
    "w2v_model.fit(\n",
    "    w2v_train_data,\n",
    "    taining_labels,\n",
    "    validation_data=(w2v_valid_data, valid_labels),\n",
    "    epochs=W2V_EPOCHS,\n",
    "    batch_size=W2V_BATCH_SIZE)\n",
    "print(\"begin evaluation\")\n",
    "w2v_model.evaluate(w2v_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-5 == 10 ** (-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T13:47:59.720958Z",
     "start_time": "2023-05-15T13:47:59.717107Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPun9gmYIf9t/BZSermdCJL",
   "include_colab_link": true,
   "mount_file_id": "1DkZU-n7fDBOIj7zec3xzfRXH0jZgeOaV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
